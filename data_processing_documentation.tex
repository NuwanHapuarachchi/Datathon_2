\documentclass[12pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{abstract}

% Code highlighting setup
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    captionpos=b,
    numbers=left,
    numberstyle=\tiny,
    frame=single,
    backgroundcolor=\color{gray!10}
}

% Page setup
\geometry{margin=1in}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Data Processing Documentation}
\fancyhead[R]{\thepage}

% Title formatting
\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\itshape}{\thesubsubsection}{1em}{}

% Colors
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\title{Data Processing Pipeline Documentation \\
       Datathon Challenge 2025 - Government Service Optimization}
\author{Data Science Team}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This document provides a comprehensive overview of the data processing pipeline developed for the Datathon Challenge 2025. The pipeline transforms raw booking and staffing data into clean, feature-rich datasets suitable for machine learning models that predict service completion times (Task 1) and staffing needs (Task 2). The documentation covers all preprocessing steps, feature engineering techniques, data validation procedures, and quality assurance measures implemented to ensure robust model performance.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

\subsection{Challenge Overview}

The Datathon Challenge 2025 focuses on optimizing government service delivery through predictive analytics. The challenge consists of two main tasks:

\begin{itemize}
    \item \textbf{Task 1}: Predict the processing time for individual service appointments before citizens arrive at government offices
    \item \textbf{Task 2}: Forecast staffing needs for each section on a given date to optimize resource allocation
\end{itemize}

Both tasks require sophisticated data processing to transform raw appointment and staffing data into actionable insights.

\subsection{Data Sources}

The project utilizes multiple datasets:

\begin{itemize}
    \item \texttt{bookings\_train.csv}: Primary training dataset containing appointment booking information
    \item \texttt{staffing\_train.csv}: Historical staffing data by section and date
    \item \texttt{tasks.csv}: Task metadata mapping tasks to organizational sections
    \item \texttt{task1\_test\_inputs.csv}: Test data for Task 1 predictions
    \item \texttt{task2\_test\_inputs.csv}: Test data for Task 2 predictions
\end{itemize}

\subsection{Processing Objectives}

The main objectives of the data processing pipeline are:

\begin{enumerate}
    \item Load and validate raw data from multiple sources
    \item Clean and preprocess temporal data
    \item Engineer relevant features for predictive modeling
    \item Handle missing data and outliers appropriately
    \item Create unified datasets ready for machine learning
    \item Perform exploratory data analysis for insights
    \item Ensure data quality and integrity throughout the pipeline
\end{enumerate}

\section{Raw Data Overview}

\subsection{Dataset Structures}

\subsubsection{Bookings Dataset}

The primary dataset contains 203,693 appointment records with the following raw structure:

\begin{table}[H]
\centering
\caption{Bookings Dataset Schema}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Column} & \textbf{Data Type} & \textbf{Description} \\
\midrule
booking\_id & string & Unique booking identifier \\
citizen\_id & int64 & Unique citizen identifier \\
booking\_date & string & Date when booking was made \\
appointment\_date & string & Scheduled appointment date \\
appointment\_time & string & Scheduled appointment time (HH:MM) \\
check\_in\_time & string & Actual check-in timestamp \\
check\_out\_time & string & Actual check-out timestamp \\
task\_id & string & Task identifier \\
num\_documents & int64 & Number of documents submitted \\
queue\_number & int64 & Queue position number \\
satisfaction\_rating & int64 & Citizen satisfaction rating (1-5) \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Staffing Dataset}

The staffing dataset contains 5,804 records of historical staffing information:

\begin{table}[H]
\centering
\caption{Staffing Dataset Schema}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Column} & \textbf{Data Type} & \textbf{Description} \\
\midrule
date & string & Date of staffing record \\
section\_id & string & Section identifier (SEC-001 to SEC-006) \\
employees\_on\_duty & int64 & Number of employees working \\
total\_task\_time\_minutes & float64 & Total processing time for section \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Tasks Metadata}

The tasks lookup table contains 20 tasks mapped to 6 sections:

\begin{table}[H]
\centering
\caption{Tasks Dataset Schema}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Column} & \textbf{Data Type} & \textbf{Description} \\
\midrule
task\_id & string & Task identifier (TASK-001 to TASK-020) \\
task\_name & string & Human-readable task name \\
section\_id & string & Section identifier \\
section\_name & string & Human-readable section name \\
\bottomrule
\end{tabular}
\end{table}

\section{Data Loading and Initial Exploration}

\subsection{Library Dependencies}

The processing pipeline utilizes the following Python libraries:

\begin{lstlisting}[language=Python, caption=Required Python Libraries]
import pandas as pd
import numpy as np
import scipy
from sklearn.preprocessing import MinMaxScaler
import seaborn as sns
import matplotlib.pyplot as plt
import math
from datetime import datetime
\end{lstlisting}

\subsection{Initial Data Loading}

The first step involves loading the primary bookings dataset and performing initial exploration:

\begin{lstlisting}[language=Python, caption=Initial Data Loading]
# Load the dataset
df = pd.read_csv('bookings_train.csv')

# Display basic information
print(df.head())
print(df.info())
print(df.isnull().sum())
print(df.describe())
\end{lstlisting}

\subsection{Data Quality Assessment}

Initial data quality checks revealed:

\begin{itemize}
    \item \textbf{Dataset Size}: 203,693 rows, 11 columns
    \item \textbf{Missing Values}: 
        \begin{itemize}
            \item check\_in\_time: 6,092 missing values (2.99\%)
            \item check\_out\_time: 6,092 missing values (2.99\%)
        \end{itemize}
    \item \textbf{Data Types}: Mixed types requiring proper parsing
    \item \textbf{Temporal Fields}: Date/time fields stored as strings
\end{itemize}

\section{Data Preprocessing and Cleaning}

\subsection{Temporal Data Parsing}

The preprocessing phase focuses on converting string-based temporal data into proper datetime objects:

\begin{lstlisting}[language=Python, caption=Temporal Data Parsing]
# Parse basic date/time columns
df['booking_date'] = pd.to_datetime(df['booking_date'], errors='coerce')
df['appointment_date'] = pd.to_datetime(df['appointment_date'], errors='coerce')

# Parse appointment_time and create appointment_datetime
df['appointment_time'] = pd.to_datetime(df['appointment_time'],
                                       format='%H:%M', errors='coerce').dt.time

# Create combined appointment datetime for feature engineering
df['appointment_datetime'] = pd.to_datetime(
    df['appointment_date'].dt.strftime('%Y-%m-%d') + ' ' +
    df['appointment_time'].astype(str),
    errors='coerce'
)

# Parse check-in/out timestamps with high precision
df['check_in_time'] = pd.to_datetime(df['check_in_time'], errors='coerce')
df['check_out_time'] = pd.to_datetime(df['check_out_time'], errors='coerce')
\end{lstlisting}

\subsection{Target Variable Creation}

The primary target variable for Task 1 is the processing time in minutes:

\begin{lstlisting}[language=Python, caption=Target Variable Computation]
# Compute processing time in minutes (target)
df['processing_time_minutes'] = (
    df['check_out_time'] - df['check_in_time']
).dt.total_seconds() / 60.0
\end{lstlisting}

\subsection{Data Filtering and Validation}

To ensure data quality, several filtering criteria are applied:

\begin{lstlisting}[language=Python, caption=Data Filtering Logic]
# Keep only reasonable, present targets
valid_mask = (
    df['processing_time_minutes'].notna()  # Must have valid processing time
    & (df['processing_time_minutes'] > 0)   # Must be positive
    & (df['processing_time_minutes'] < 480) # Must be less than 8 hours
)

df_task1 = df.loc[valid_mask].copy()
print(f'Rows before: {len(df)} | After filtering: {len(df_task1)}')
\end{lstlisting}

\textbf{Filtering Results}:
\begin{itemize}
    \item Original records: 203,693
    \item Filtered records: 197,601
    \item Records removed: 6,092 (due to missing timestamps)
\end{itemize}

\section{Feature Engineering}

\subsection{Metadata Integration}

Task and section information is integrated from the metadata:

\begin{lstlisting}[language=Python, caption=Metadata Integration]
# Load tasks metadata
tasks = pd.read_csv('tasks.csv')

# Merge on task_id to bring task/section names
df_task1 = df_task1.merge(tasks, on='task_id', how='left')

# Validate merge integrity
print(f"Rows missing task metadata: {df_task1['task_name'].isna().sum()}")
print(f"Unique sections: {df_task1['section_id'].nunique()}")
\end{lstlisting}

\subsection{Temporal Feature Engineering}

Rich temporal features are extracted from appointment datetime:

\begin{lstlisting}[language=Python, caption=Temporal Feature Extraction]
# Extract time-based features
df_task1['appt_hour'] = df_task1['appointment_datetime'].dt.hour
df_task1['appt_minute'] = df_task1['appointment_datetime'].dt.minute
df_task1['appt_dayofweek'] = df_task1['appointment_datetime'].dt.dayofweek  # 0=Monday
df_task1['appt_month'] = df_task1['appointment_datetime'].dt.month
\end{lstlisting}

\subsection{Cyclical Time Encoding}

To capture the cyclical nature of time, sine and cosine transformations are applied:

\begin{lstlisting}[language=Python, caption=Cyclical Time Encoding]
def encode_cyclical(series, period):
    """Encode cyclical features using sine and cosine transformations."""
    return np.sin(2 * np.pi * series / period), np.cos(2 * np.pi * series / period)

# Cyclical encoding for hour (24-hour period)
hour_sin, hour_cos = encode_cyclical(df_task1['appt_hour'], 24)
df_task1['hour_sin'] = hour_sin
df_task1['hour_cos'] = hour_cos

# Cyclical encoding for minute (60-minute period)
minute_sin, minute_cos = encode_cyclical(df_task1['appt_minute'], 60)
df_task1['minute_sin'] = minute_sin
df_task1['minute_cos'] = minute_cos
\end{lstlisting}

\subsection{Queue-Based Features}

Queue density and position features are engineered:

\begin{lstlisting}[language=Python, caption=Queue Feature Engineering]
# Compute per-day, per-section queue statistics
daily_section_queue = (
    df_task1.groupby([df_task1['appointment_date'], 'section_id'])['queue_number']
    .agg(['count', 'max'])
    .rename(columns={'count': 'daily_section_count', 'max': 'daily_section_queue_max'})
    .reset_index()
)

# Merge queue statistics back to main dataset
df_task1 = df_task1.merge(
    daily_section_queue,
    left_on=['appointment_date', 'section_id'],
    right_on=['appointment_date', 'section_id'],
    how='left'
)

# Compute normalized queue position
df_task1['queue_position_ratio'] = (
    df_task1['queue_number'] / df_task1['daily_section_queue_max']
)
\end{lstlisting}

\subsection{Document Load Feature}

A binary feature captures high document load:

\begin{lstlisting}[language=Python, caption=Document Load Feature]
# Create binary feature for high document load
df_task1['has_many_documents'] = (df_task1['num_documents'] >= 3).astype(int)
\end{lstlisting}

\section{Final Dataset Preparation}

\subsection{Feature Selection}

The final feature set is carefully selected for Task 1 modeling:

\begin{lstlisting}[language=Python, caption=Feature Selection]
feature_cols = [
    'processing_time_minutes',  # Target variable
    'task_id', 'section_id', 'task_name', 'section_name',  # Task metadata
    'num_documents', 'queue_number', 'queue_position_ratio',  # Queue features
    'daily_section_count', 'daily_section_queue_max',  # Daily statistics
    'appt_hour', 'appt_minute', 'appt_dayofweek', 'appt_month',  # Temporal
    'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos'  # Cyclical features
]

# Ensure all columns exist
available_cols = [c for c in feature_cols if c in df_task1.columns]
df_task1_clean = df_task1[available_cols].copy()
\end{lstlisting}

\subsection{Dataset Export}

The cleaned dataset is saved for modeling:

\begin{lstlisting}[language=Python, caption=Dataset Export]
output_path = 'task1_features_clean.csv'
df_task1_clean.to_csv(output_path, index=False)
print(f'Saved cleaned Task 1 dataset to {output_path} with shape {df_task1_clean.shape}')
\end{lstlisting}

\textbf{Final Dataset Characteristics}:
\begin{itemize}
    \item Shape: (197,601, 18)
    \item Target variable: processing\_time\_minutes
    \item Features: 17 engineered features
    \item No missing values in final dataset
\end{itemize}

\section{Exploratory Data Analysis}

\subsection{Data Overview and Statistics}

Comprehensive statistics are computed for the cleaned dataset:

\begin{lstlisting}[language=Python, caption=Dataset Statistics]
# Basic dataset information
print(f"Dataset Shape: {df_clean.shape}")
print(f"Total Rows: {len(df_clean):,}")
print(f"Total Columns: {len(df_clean.columns)}")

# Missing values analysis
missing_data = df_clean.isnull().sum()
missing_percent = (missing_data / len(df_clean)) * 100
missing_df = pd.DataFrame({
    'Missing_Count': missing_data,
    'Missing_Percent': missing_percent
})

# Descriptive statistics for numerical columns
numerical_cols = df_clean.select_dtypes(include=[np.number]).columns
print(df_clean[numerical_cols].describe())
\end{lstlisting}

\subsection{Target Variable Analysis}

Detailed analysis of the target variable reveals:

\begin{table}[H]
\centering
\caption{Processing Time Statistics}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Count & 197,601 \\
Mean & 48.81 minutes \\
Standard Deviation & 24.00 minutes \\
Median & 42.80 minutes \\
Minimum & 5.92 minutes \\
Maximum & 217.64 minutes \\
10th Percentile & 21.45 minutes \\
25th Percentile & 31.85 minutes \\
75th Percentile & 59.96 minutes \\
90th Percentile & 81.59 minutes \\
95th Percentile & 97.66 minutes \\
99th Percentile & 129.17 minutes \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Categorical Variable Analysis}

Analysis of categorical variables shows:

\begin{itemize}
    \item \textbf{Task ID}: 20 unique tasks
    \item \textbf{Section ID}: 6 sections with varying workloads:
        \begin{itemize}
            \item SEC-004: 43,354 appointments (highest)
            \item SEC-006: 19,852 appointments (lowest)
        \end{itemize}
    \item \textbf{Task Names}: All placeholder names
    \item \textbf{Section Names}: All placeholder names
\end{itemize}

\subsection{Visual Exploratory Analysis}

Several visualizations are generated to understand data patterns:

\begin{enumerate}
    \item \textbf{Target Distribution}: Histogram showing right-skewed distribution
    \item \textbf{Section-wise Analysis}: Box plots comparing processing times across sections
    \item \textbf{Document Impact}: Scatter plots showing relationship between document count and processing time
    \item \textbf{Hourly Patterns}: Analysis of processing time variation by appointment hour
\end{enumerate}

\subsection{Feature Correlation Analysis}

Scatter plots are generated for all numerical features against the target variable to identify:

\begin{itemize}
    \item Linear relationships
    \item Non-linear patterns
    \item Potential outliers
    \item Feature importance signals
\end{itemize}

\section{Data Quality Assurance}

\subsection{Validation Checks}

Comprehensive validation ensures data integrity:

\begin{enumerate}
    \item \textbf{Missing Value Verification}: Confirmed no missing values in final dataset
    \item \textbf{Data Type Validation}: All columns have appropriate data types
    \item \textbf{Range Validation}: Target variable within reasonable bounds (0-480 minutes)
    \item \textbf{Feature Consistency}: All engineered features computed correctly
    \item \textbf{Merge Integrity}: All task metadata successfully joined
\end{enumerate}

\subsection{Data Distribution Analysis}

Distribution analysis confirms:

\begin{itemize}
    \item Target variable follows expected right-skewed distribution
    \item Queue numbers are reasonably distributed
    \item Temporal features capture expected patterns
    \item Section workloads show realistic variation
\end{itemize}

\section{Data Insights and Findings}

\subsection{Key Findings}

From the exploratory analysis, several important insights emerge:

\begin{enumerate}
    \item \textbf{Processing Time Distribution}: Most appointments complete within 30-60 minutes
    \item \textbf{Section Variability}: Different sections have distinct processing time patterns
    \item \textbf{Queue Effects}: Queue position and daily volume impact processing times
    \item \textbf{Temporal Patterns}: Clear hourly and daily patterns in processing efficiency
    \item \textbf{Document Impact}: Higher document counts correlate with longer processing times
\end{enumerate}

\subsection{Model Readiness Assessment}

The processed dataset is well-suited for machine learning:

\begin{itemize}
    \item \textbf{Feature Richness}: 17 engineered features capture multiple dimensions
    \item \textbf{Target Quality}: Well-defined, continuous target variable
    \item \textbf{Data Volume}: 197,601 training samples provide statistical power
    \item \textbf{Feature Types}: Mix of categorical, numerical, and temporal features
    \item \textbf{Quality}: Clean dataset with no missing values or data quality issues
\end{itemize}

\section{Conclusion and Next Steps}

\subsection{Processing Pipeline Summary}

The data processing pipeline successfully transforms raw booking data into a clean, feature-rich dataset suitable for predictive modeling. Key achievements include:

\begin{itemize}
    \item Successfully processed 203,693 raw records
    \item Created 197,601 clean training samples
    \item Engineered 17 predictive features
    \item Implemented robust data validation and quality checks
    \item Generated comprehensive exploratory analysis
\end{itemize}

\subsection{Task 1 Model Input Specification}

The final dataset (\texttt{task1\_features\_clean.csv}) is ready for Task 1 modeling with:

\begin{itemize}
    \item \textbf{Target}: \texttt{processing\_time\_minutes} (continuous)
    \item \textbf{Key Features}: 
        \begin{itemize}
            \item Temporal: hour, minute, day of week, month
            \item Cyclical: sine/cosine encoded time features
            \item Queue: position, ratio, daily counts
            \item Task: task\_id, section\_id, metadata
            \item Documents: count and binary high-load indicator
        \end{itemize}
\end{itemize}

\subsection{Recommendations for Modeling}

Based on the data analysis, the following modeling approaches are recommended:

\begin{enumerate}
    \item \textbf{Ensemble Methods}: Random Forest, Gradient Boosting for capturing non-linear relationships
    \item \textbf{Time Series Models}: Consider temporal patterns in processing times
    \item \textbf{Feature Selection}: Use correlation analysis to identify most predictive features
    \item \textbf{Validation Strategy}: Implement time-based cross-validation to respect temporal ordering
    \item \textbf{Error Metrics}: Use RMSE and MAE for evaluation, with business-focused metrics
\end{enumerate}

\subsection{Future Enhancements}

Potential improvements to the processing pipeline:

\begin{itemize}
    \item \textbf{Advanced Feature Engineering}: Interaction terms, polynomial features
    \item \textbf{External Data Integration}: Weather, holidays, seasonal patterns
    \item \textbf{Real-time Processing}: Streaming data processing capabilities
    \item \textbf{Automated Pipeline}: Scheduled data updates and model retraining
    \item \textbf{Model Interpretability}: SHAP values, feature importance analysis
\end{itemize}

This comprehensive data processing pipeline provides a solid foundation for developing accurate predictive models for both Task 1 (processing time prediction) and Task 2 (staffing optimization) of the Datathon Challenge 2025.

\end{document}
